# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import os
from dotenv import load_dotenv
import boto3
from typing import TypedDict, Annotated
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from langchain_aws import ChatBedrock
from langchain_core.messages import HumanMessage, AIMessage
from langchain_community.tools.tavily_search import TavilySearchResults

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€
load_dotenv()

# çŠ¶æ…‹ã®å®šç¾©
class State(TypedDict):
    messages: Annotated[list, add_messages]
    query: str
    agent_type: str
    context: str

# ãƒãƒ£ãƒƒãƒˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆæ—¢å­˜çŸ¥è­˜ã§å›ç­”ï¼‰
def chat_agent(state: State):
    """ä¸€èˆ¬çš„ãªè³ªå•ã«æ—¢å­˜ã®çŸ¥è­˜ã§å›ç­”ã—ã¾ã™"""
    print("ğŸ¥ chat_agentå‘¼ã³å‡ºã•ã‚Œã¾ã—ãŸ")
    
    llm = ChatBedrock(
        # model_id="us.anthropic.claude-3-5-haiku-20241022-v1:0",
        model_id="us.anthropic.claude-3-5-haiku-20241022-v1:0",
        model_kwargs={"max_tokens": 300}
    )
    
    response = llm.invoke([HumanMessage(content=f"ç°¡æ½”ã§åˆ†ã‹ã‚Šã‚„ã™ãå›ç­”ã—ã¦ãã ã•ã„ã€‚\n\nè³ªå•: {state['query']}")])
    
    return {
        "messages": [AIMessage(content=response.content)],
        "context": response.content
    }

# Webæ¤œç´¢ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆTavilyä½¿ç”¨ï¼‰
def web_search_agent(state: State):
    """Webæ¤œç´¢ã‚’ä½¿ã£ã¦æœ€æ–°æƒ…å ±ã‚’èª¿ã¹ã¦å›ç­”ã—ã¾ã™"""
    print("ğŸŒ web_search_agentå‘¼ã³å‡ºã•ã‚Œã¾ã—ãŸ")
    
    query = state['query']
    
    try:
        # Tavilyæ¤œç´¢ãƒ„ãƒ¼ãƒ«ã‚’åˆæœŸåŒ–
        search = TavilySearchResults(
            max_results=5,
            api_key=os.getenv("TAVILY_API_KEY")
        )
        
        # Webæ¤œç´¢å®Ÿè¡Œ
        print(f"ğŸ” Webæ¤œç´¢å®Ÿè¡Œ: {query}")
        search_results = search.invoke({"query": query})
        
        # æ¤œç´¢çµæœã‚’æ•´ç†
        context = "Webæ¤œç´¢çµæœ:\n"
        for i, result in enumerate(search_results[:3]):  # ä¸Šä½3ä»¶
            title = result.get('title', '')
            content = result.get('content', '')
            url = result.get('url', '')
            context += f"\n{i+1}. {title}\n{content[:200]}...\nURL: {url}\n"
        
        print(f"âœ… Webæ¤œç´¢æˆåŠŸ: {len(search_results)}ä»¶ã®çµæœ")
        
        # æ¤œç´¢çµæœã‚’åŸºã«å›ç­”ç”Ÿæˆ
        print("ğŸ¤– å›ç­”ç”Ÿæˆä¸­...")
        llm = ChatBedrock(
            model_id="us.anthropic.claude-3-5-haiku-20241022-v1:0",
            model_kwargs={"max_tokens": 500}
        )
        
        prompt = f"""ä»¥ä¸‹ã®Webæ¤œç´¢çµæœã‚’åŸºã«ã€è³ªå•ã«ç°¡æ½”ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã®ã¿å«ã‚ã¦ãã ã•ã„ã€‚

{context}

è³ªå•: {query}"""
        
        response = llm.invoke([HumanMessage(content=prompt)])
        
        return {
            "messages": [AIMessage(content=response.content)],
            "context": response.content
        }
        
    except Exception as e:
        print(f"âŒ Webæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return {
            "messages": [AIMessage(content=f"Webæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")],
            "context": f"Webæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}"
        }

# Knowledge Baseã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
def knowledge_base_agent(state: State):
    """Knowledge Baseã‹ã‚‰é–¢é€£æƒ…å ±ã‚’æ¤œç´¢ã—ã¦å›ç­”ã—ã¾ã™"""
    print('ğŸ“š Knowledge Baseã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒå‘¼ã³å‡ºã•ã‚Œã¾ã—ãŸ')
    
    query = state['query']
    
    # Bedrock Agent Runtimeã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
    kb_region = os.getenv('KNOWLEDGE_BASE_REGION', 'ap-northeast-1')
    bedrock_agent = boto3.client('bedrock-agent-runtime', region_name=kb_region)
    
    try:
        print(f"ğŸ” Knowledge Base ID: {os.getenv('KNOWLEDGE_BASE_ID')}")
        print(f"ğŸ” æ¤œç´¢ã‚¯ã‚¨ãƒª: {query}")
        
        # Knowledge Baseã‹ã‚‰æ¤œç´¢
        try:
            kb_response = bedrock_agent.retrieve(
                knowledgeBaseId=os.getenv('KNOWLEDGE_BASE_ID'),
                retrievalQuery={'text': query},
                retrievalConfiguration={
                    'vectorSearchConfiguration': {
                        'numberOfResults': 10,
                        'overrideSearchType': 'HYBRID',
                        'rerankingConfiguration': {
                            'bedrockRerankingConfiguration': {
                                'modelConfiguration': {
                                    'modelArn': f'arn:aws:bedrock:{kb_region}::foundation-model/amazon.rerank-v1:0'
                                },
                                'numberOfRerankedResults': 3,
                            },
                            "type": "BEDROCK_RERANKING_MODEL"
                        }
                    }
                }
            )
            print("ğŸ”„ ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ + Amazon Rerank v1.0 ä½¿ç”¨")
        except Exception as rerank_error:
            print(f"âš ï¸ ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {rerank_error}")
            kb_response = bedrock_agent.retrieve(
                knowledgeBaseId=os.getenv('KNOWLEDGE_BASE_ID'),
                retrievalQuery={'text': query},
                retrievalConfiguration={
                    'vectorSearchConfiguration': {
                        'numberOfResults': 5,
                        'overrideSearchType': 'HYBRID'
                    }
                }
            )
            print("ğŸ”„ ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ä½¿ç”¨")
        
        print(f"âœ… Knowledge Baseæ¤œç´¢æˆåŠŸ")
        print(f"ğŸ“Š æ¤œç´¢çµæœæ•°: {len(kb_response.get('retrievalResults', []))}")
        
        # æ¤œç´¢çµæœã‚’æ•´ç†
        context = ""
        for i, result in enumerate(kb_response.get('retrievalResults', [])):
            content = result.get('content', {}).get('text', '')
            score = result.get('score', 0)
            print(f"ğŸ“„ çµæœ{i+1} (ã‚¹ã‚³ã‚¢: {score:.3f}): {content[:100]}...")
            context += f"- {content}\n"
        
        if not context:
            print("âš ï¸ Knowledge Baseã«é–¢é€£æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return {
                "messages": [AIMessage(content="Knowledge Baseã«é–¢é€£ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")],
                "context": "Knowledge Baseã«é–¢é€£ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
            }
        
        # æ¤œç´¢çµæœã‚’åŸºã«å›ç­”ç”Ÿæˆ
        print("ğŸ¤– å›ç­”ç”Ÿæˆä¸­...")
        llm = ChatBedrock(
            # model_id="us.anthropic.claude-sonnet-4-20250514-v1:0",
            model_id="us.anthropic.claude-3-5-haiku-20241022-v1:0",
            model_kwargs={
                "temperature": 0.0,
                "max_tokens": 1024,
                "top_p": 0.1,
                "top_k": 1
            }
        )
        
        prompt = f"ä»¥ä¸‹ã®Knowledge Baseã®æƒ…å ±ã‚’åŸºã«ã€è³ªå•ã«ç°¡æ½”ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚\n\n{context}\n\nè³ªå•: {query}"
        response = llm.invoke([HumanMessage(content=prompt)])
        
        return {
            "messages": [AIMessage(content=response.content)],
            "context": response.content
        }
        
    except Exception as e:
        print(f"âŒ Knowledge Baseã‚¨ãƒ©ãƒ¼: {str(e)}")
        return {
            "messages": [AIMessage(content=f"Knowledge Baseæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")],
            "context": f"Knowledge Baseæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}"
        }

# AWSã‚³ã‚¹ãƒˆç®¡ç†ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
def aws_cost_agent(state: State):
    """AWSã®ã‚³ã‚¹ãƒˆæƒ…å ±ã‚’å–å¾—ã—ã¦å›ç­”ã—ã¾ã™"""
    print('ğŸ’° aws_cost_agentå‘¼ã³å‡ºã•ã‚Œã¾ã—ãŸ')
    
    query = state['query']
    
    try:
        # Cost Explorerã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        cost_client = boto3.client('ce', region_name='us-east-1')  # Cost Explorerã¯ us-east-1 ã®ã¿
        
        print(f"ğŸ’° ã‚³ã‚¹ãƒˆæƒ…å ±å–å¾—ä¸­: {query}")
        
        from datetime import datetime, timedelta
        
        # éå»30æ—¥é–“ã®ã‚³ã‚¹ãƒˆå–å¾—
        end_date = datetime.now().date()
        start_date = end_date - timedelta(days=30)
        
        # æ—¥åˆ¥ã‚³ã‚¹ãƒˆå–å¾—
        daily_cost_response = cost_client.get_cost_and_usage(
            TimePeriod={
                'Start': start_date.strftime('%Y-%m-%d'),
                'End': end_date.strftime('%Y-%m-%d')
            },
            Granularity='DAILY',
            Metrics=['BlendedCost']
        )
        
        # ã‚µãƒ¼ãƒ“ã‚¹åˆ¥ã‚³ã‚¹ãƒˆå–å¾—
        service_cost_response = cost_client.get_cost_and_usage(
            TimePeriod={
                'Start': start_date.strftime('%Y-%m-%d'),
                'End': end_date.strftime('%Y-%m-%d')
            },
            Granularity='MONTHLY',
            Metrics=['BlendedCost'],
            GroupBy=[
                {
                    'Type': 'DIMENSION',
                    'Key': 'SERVICE'
                }
            ]
        )
        
        # ã‚³ã‚¹ãƒˆæƒ…å ±ã‚’æ•´ç†
        cost_info = "AWSã‚³ã‚¹ãƒˆæƒ…å ± (éå»30æ—¥é–“):\n\n"
        
        # ç·ã‚³ã‚¹ãƒˆè¨ˆç®—
        total_cost = 0
        daily_costs = []
        for result in daily_cost_response['ResultsByTime']:
            daily_amount = float(result['Total']['BlendedCost']['Amount'])
            total_cost += daily_amount
            daily_costs.append({
                'date': result['TimePeriod']['Start'],
                'cost': daily_amount
            })
        
        cost_info += f"ğŸ“Š ç·ã‚³ã‚¹ãƒˆ: ${total_cost:.2f}\n"
        cost_info += f"ğŸ“… æœŸé–“: {start_date} ï½ {end_date}\n\n"
        
        # æœ€è¿‘5æ—¥é–“ã®ã‚³ã‚¹ãƒˆ
        cost_info += "ğŸ“ˆ æœ€è¿‘5æ—¥é–“ã®æ—¥åˆ¥ã‚³ã‚¹ãƒˆ:\n"
        for daily in daily_costs[-5:]:
            cost_info += f"  {daily['date']}: ${daily['cost']:.2f}\n"
        cost_info += "\n"
        
        # ã‚µãƒ¼ãƒ“ã‚¹åˆ¥ã‚³ã‚¹ãƒˆï¼ˆä¸Šä½5ã¤ï¼‰
        cost_info += "ğŸ”§ ã‚µãƒ¼ãƒ“ã‚¹åˆ¥ã‚³ã‚¹ãƒˆ (ä¸Šä½5ã¤):\n"
        if service_cost_response['ResultsByTime']:
            groups = service_cost_response['ResultsByTime'][0]['Groups']
            # ã‚³ã‚¹ãƒˆé †ã§ã‚½ãƒ¼ãƒˆ
            sorted_services = sorted(groups, key=lambda x: float(x['Metrics']['BlendedCost']['Amount']), reverse=True)
            
            for i, service in enumerate(sorted_services[:5]):
                service_name = service['Keys'][0]
                service_cost = float(service['Metrics']['BlendedCost']['Amount'])
                cost_info += f"  {i+1}. {service_name}: ${service_cost:.2f}\n"
        
        print(f"âœ… ã‚³ã‚¹ãƒˆæƒ…å ±å–å¾—æˆåŠŸ")
        print(f"ğŸ’° ç·ã‚³ã‚¹ãƒˆ: ${total_cost:.2f}")
        
        # ã‚³ã‚¹ãƒˆæƒ…å ±ã‚’åŸºã«å›ç­”ç”Ÿæˆ
        print("ğŸ¤– ã‚³ã‚¹ãƒˆåˆ†æå›ç­”ç”Ÿæˆä¸­...")
        llm = ChatBedrock(
            model_id="us.anthropic.claude-3-5-haiku-20241022-v1:0",
            model_kwargs={
                "temperature": 0.0,
                "max_tokens": 1024
            }
        )
        
        prompt = f"""ä»¥ä¸‹ã®AWSã‚³ã‚¹ãƒˆæƒ…å ±ã‚’åŸºã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚ã‚³ã‚¹ãƒˆåˆ†æã‚„ç¯€ç´„ææ¡ˆã‚‚å«ã‚ã¦ãã ã•ã„ã€‚

{cost_info}

è³ªå•: {query}"""
        
        response = llm.invoke([HumanMessage(content=prompt)])
        
        return {
            "messages": [AIMessage(content=response.content)],
            "context": response.content
        }
        
    except Exception as e:
        print(f"âŒ AWSã‚³ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {str(e)}")
        return {
            "messages": [AIMessage(content=f"AWSã‚³ã‚¹ãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")],
            "context": f"AWSã‚³ã‚¹ãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}"
        }

# ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼ï¼ˆã©ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ä½¿ã†ã‹åˆ¤æ–­ï¼‰
def orchestrator(state: State):
    """è³ªå•ã‚’åˆ†æã—ã¦é©åˆ‡ãªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’é¸æŠ"""
    query = state['query']
    
    llm = ChatBedrock(
        model_id="us.anthropic.claude-3-5-haiku-20241022-v1:0",
        model_kwargs={"max_tokens": 100}
    )
    
    prompt = f"""ä»¥ä¸‹ã®è³ªå•ã‚’åˆ†æã—ã¦ã€é©åˆ‡ãªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚

è³ªå•: {query}

é¸æŠè‚¢:
- chat: é€šå¸¸ã®å›ç­”
- web_search: Webæ¤œç´¢ãŒå¿…è¦ãªæœ€æ–°æƒ…å ±ã®è³ªå•
- knowledge_base: ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã‚’ä½¿ã£ãŸå›ç­”ï¼ˆã‚¸ãƒ§ã‚¸ãƒ§ã®å¥‡å¦™ãªå†’é™ºã€AWSç³»ã®è³ªå•ï¼‰
- aws_cost: AWSã®ã‚³ã‚¹ãƒˆãƒ»æ–™é‡‘ãƒ»è«‹æ±‚ã«é–¢ã™ã‚‹è³ªå•

å›ç­”ã¯ã€Œchatã€ã€ã€Œweb_searchã€ã€ã€Œknowledge_baseã€ã€ã¾ãŸã¯ã€Œaws_costã€ã®ã¿ã§ç­”ãˆã¦ãã ã•ã„ã€‚"""
    
    response = llm.invoke([HumanMessage(content=prompt)])
    agent_type = response.content.strip().lower()
    
    if "knowledge_base" in agent_type:
        agent_type = "knowledge_base"
    elif "web_search" in agent_type:
        agent_type = "web_search"
    elif "aws_cost" in agent_type:
        agent_type = "aws_cost"
    else:
        agent_type = "chat"
    
    print(f"ğŸ¯ é¸æŠã•ã‚ŒãŸã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ: {agent_type}")
    
    return {"agent_type": agent_type}

# æ¡ä»¶åˆ†å²é–¢æ•°
def route_agent(state: State):
    """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚¿ã‚¤ãƒ—ã«åŸºã¥ã„ã¦ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°"""
    if state["agent_type"] == "knowledge_base":
        return "knowledge_base_agent"
    elif state["agent_type"] == "web_search":
        return "web_search_agent"
    elif state["agent_type"] == "aws_cost":
        return "aws_cost_agent"
    else:
        return "chat_agent"

# ã‚°ãƒ©ãƒ•ã®æ§‹ç¯‰
def create_graph():
    workflow = StateGraph(State)
    
    # ãƒãƒ¼ãƒ‰ã‚’è¿½åŠ ï¼ˆãƒãƒ¼ãƒ‰ã¨ã¯ã€ï¼‘ã¤ã®ä½œæ¥­ã‚’è¡Œã†ç®±ã€‚é–¢æ•°ã‚’å®Ÿè¡Œã™ã‚‹å ´æ‰€ã€‚ãƒ‡ãƒ¼ã‚¿ã‚’å—ã‘å–ã£ã¦ã€å‡¦ç†ã—ã¦ã€çµæœã‚’è¿”ã™ã‚‚ã®ï¼‰
    workflow.add_node("orchestrator", orchestrator)
    workflow.add_node("chat_agent", chat_agent)
    workflow.add_node("web_search_agent", web_search_agent)
    workflow.add_node("knowledge_base_agent", knowledge_base_agent)
    workflow.add_node("aws_cost_agent", aws_cost_agent)
    
    # ã‚¨ãƒƒã‚¸ã‚’è¿½åŠ ï¼ˆã‚¨ãƒƒã‚¸ã¨ã¯ã€ãƒãƒ¼ãƒ‰é–“ã®é“è·¯ãƒ»çŸ¢å°ã€‚ãƒãƒ¼ãƒ‰ã‹ã‚‰ãƒãƒ¼ãƒ‰ã¸ã®ç§»å‹•çµŒè·¯ã€å®Ÿè¡Œé †åºã‚’æ±ºã‚ã‚‹ã€ãƒ‡ãƒ¼ã‚¿ã®æµã‚Œã‚’åˆ¶å¾¡ã™ã‚‹ã‚‚ã®ï¼‰
    workflow.set_entry_point("orchestrator") # set_entry_pointã§ã‚°ãƒ©ãƒ•å®Ÿè¡Œæ™‚ã«æœ€åˆã«å®Ÿè¡Œã•ã‚Œã‚‹ãƒãƒ¼ãƒ‰ã‚’æŒ‡å®šã€‚å¿…ãšï¼‘ã¤æŒ‡å®šã™ã‚‹ã€‚
    workflow.add_conditional_edges(
        "orchestrator",
        route_agent,
        {
            "chat_agent": "chat_agent",
            "web_search_agent": "web_search_agent",
            "knowledge_base_agent": "knowledge_base_agent",
            "aws_cost_agent": "aws_cost_agent"
        }
    )
    workflow.add_edge("chat_agent", END)
    workflow.add_edge("web_search_agent", END)
    workflow.add_edge("knowledge_base_agent", END)
    workflow.add_edge("aws_cost_agent", END)
    
    return workflow.compile()

# ãƒ¬ã‚¹ãƒãƒ³ã‚¹å–å¾—é–¢æ•°
def get_response(question: str):
    """è³ªå•ã«å¯¾ã™ã‚‹å›ç­”ã‚’å–å¾—"""
    graph = create_graph()
    
    result = graph.invoke({
        "messages": [],
        "query": question,
        "agent_type": "",
        "context": ""
    })
    
    return result["context"]

# ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒ£ãƒƒãƒˆ
if __name__ == "__main__":
    print("=== AI ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ (LangGraphç‰ˆ) ===")
    print("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆ'quit'ã§çµ‚äº†ï¼‰")
    print("="*40)
    
    while True:
        try:
            question = input("\nè³ªå•: ")
            
            if question.lower() in ['quit', 'exit', 'çµ‚äº†']:
                print("ãƒãƒ£ãƒƒãƒˆã‚’çµ‚äº†ã—ã¾ã™ã€‚")
                break
                
            if not question.strip():
                continue
                
            print("\nå›ç­”ä¸­...")
            result = get_response(question)
            print(f"\nå›ç­”: {result}")
            
        except KeyboardInterrupt:
            print("\n\nãƒãƒ£ãƒƒãƒˆã‚’çµ‚äº†ã—ã¾ã™ã€‚")
            break
        except Exception as e:
            print(f"\nã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            print("ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚")